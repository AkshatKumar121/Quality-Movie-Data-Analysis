ğŸ“½ï¸ Quality Movie Data Analysis
An Industrial-Grade Data Ingestion & Quality Processing Pipeline using AWS
ğŸš€ Project Overview

The Quality Movie Data Analysis project focuses on designing and implementing a scalable, automated, and quality-driven data ingestion pipeline using AWS services.
The goal is to ingest only high-quality movie data into the Amazon Redshift Data Warehouse by validating datasets at every stage and ensuring only trusted data is available for analytics.

ğŸ—ï¸ Architecture

Tech Stack Used:

Amazon S3 â€“ Raw & processed movie dataset storage

AWS Glue Crawler â€“ Schema inference & table creation in Glue Catalog

AWS Glue Data Catalog â€“ Central metadata store

AWS Glue Data Quality â€“ Validation and data-quality rule checks

AWS Glue Low-Code ETL â€“ Data transformations & quality filtering

Amazon Redshift â€“ Final analytical data warehouse

Amazon EventBridge â€“ Automated pipeline triggering

Amazon SNS â€“ Notification for pipeline success/failure

ğŸ“‚ Folder Structure
Quality-Movie-Data-Analysis/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                 # Raw movie dataset stored in S3
â”‚   â”œâ”€â”€ processed/           # Cleaned and high-quality data
â”‚
â”œâ”€â”€ glue-scripts/
â”‚   â”œâ”€â”€ etl_quality_job.py   # Low-code + custom transformations
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ dq_rules.json        # Glue Data Quality rule set
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ redshift_schema.sql  # Table creation for Redshift
â”‚
â””â”€â”€ README.md

ğŸ”§ Pipeline Workflow
1ï¸âƒ£ Upload Data to S3

Raw movie files (CSV/JSON/Parquet) are uploaded to the S3 raw bucket.

2ï¸âƒ£ Cataloging with Glue Crawler

Glue Crawler scans the S3 raw file

Automatically detects schema

Creates metadata tables in Glue Data Catalog

3ï¸âƒ£ Apply Data Quality Checks

Using Glue Data Quality, rules such as:

Non-null checks

Valid range constraints (ratings, release year)

Referential checks

Duplicate record checks

Only high-quality records pass through to the next stage.

4ï¸âƒ£ Transform using Glue Low-Code ETL

Clean bad records

Standardize column formats

Filter invalid rows

Generate curated dataset

5ï¸âƒ£ Load High-Quality Data into Redshift

Curated dataset is inserted into Redshift fact & dimension tables.

6ï¸âƒ£ EventBridge Automation

EventBridge triggers the Glue ETL job when:

New files land in S3

Scheduled ingestion events occur

7ï¸âƒ£ SNS Notifications

SNS sends success/failure alerts to email subscribers.

ğŸ“Š Outcome

âœ” Ensures only validated, high-quality movie data reaches Redshift
âœ” Automated ingestion pipeline requiring no manual intervention
âœ” Metadata-driven processing with schema evolution support
âœ” Fully scalable and serverless architecture

ğŸ› ï¸ How to Run the Project

Upload your movie dataset to the S3 raw bucket.

Run the Glue Crawler to create/update metadata.

Trigger the Glue ETL Quality Job manually or via EventBridge.

After successful completion, query the processed data in Amazon Redshift.